{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa84cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cd9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Skew\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f024d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| Name|Age|    Education|\n",
      "+-----+---+-------------+\n",
      "|  Raj| 25|   MBA,BE,HSC|\n",
      "|Shyam| 32|         NULL|\n",
      "|Madhu| 47|ME,BE,Diploma|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql adaptive query execution adaptive.coalescePartitions.enabled will makr paritions dynamic\n",
    "sc.setLogLevel(\"Error\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",3)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"false\")\n",
    "\n",
    "\"\"\"\n",
    "Skewness unequal distribution of data across cluster\n",
    "Skewness is handled by adpative query exuection in 3.0. Skewness(Spark 3.0) splits the data\n",
    "In version 2.0 - handled in following ways\n",
    "1. Repartition (increase partition size; will not be effective for single dominant partition)\n",
    "2. Salting technique\n",
    "3. Reducing date skew effect before uploading(avoid selecting skewed parition id(like data column))\n",
    "4. Bump up spark.sql.autoBroadcastJoinThreshold\n",
    "5. Iterative(chunked) join broadcast\n",
    "\"\"\"\n",
    "filepath = \"file:///C:/Users/venka/PycharmProjects/pythonProject/dataset/\"\n",
    "df = spark.read.option(\"delimiter\",\"|\").option(\"header\",True).csv(filepath + \"IntExplode.csv\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
