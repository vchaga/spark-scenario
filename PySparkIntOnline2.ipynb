{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa84cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cd9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Online2\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f024d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql adaptive query execution adaptive.coalescePartitions.enabled will makr paritions dynamic\n",
    "sc.setLogLevel(\"Error\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",3)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\",True)\n",
    "\n",
    "\"\"\"\n",
    "Online coding question\n",
    "1.Read the file as spark RDD not DF\n",
    "2.Filter header from RDD\n",
    "3.Calcualte final price (price = size * price persqft)\n",
    "4.save the file to textfile with header as pipe delimited\n",
    "\n",
    "#Since we need a single text file as O/P we use coalesce of 1 , which merges all part files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#WithoutInferSchema\n",
    "#headerTrue will read first row and assign column names but type is String for all\n",
    "#spark job created to read first column\n",
    "filepath = \"file:///C:/Users/venka/PycharmProjects/pythonProject/dataset/\"\n",
    "df = spark.read.option(\"header\",True) \\\n",
    "            .option(\"inferSchema\",True) \\\n",
    "            .option(\"delimiter\",\",\") \\\n",
    "            .csv(filepath + \"IntPersonal_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3a47dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sc.textFile(filepath + \"IntRealEstate.txt\") \\\n",
    "        .mapPartitionsWithIndex(lambda idx, iter: list(iter)[1:] if (idx==0) else iter) \\\n",
    "        .map(lambda x: x.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d2d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db66c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1461262',\n",
       "  'Arroyo Grande',\n",
       "  '795000',\n",
       "  '3',\n",
       "  '3',\n",
       "  '2371',\n",
       "  '365.3',\n",
       "  'Short Sale'],\n",
       " ['1478004',\n",
       "  'Paulo Pablo',\n",
       "  '399000',\n",
       "  '4',\n",
       "  '3',\n",
       "  '2818',\n",
       "  '163.59',\n",
       "  'Short Sale'],\n",
       " ['1486551',\n",
       "  'Paulo Pablo',\n",
       "  '545000',\n",
       "  '4',\n",
       "  '3',\n",
       "  '3032',\n",
       "  '179.75',\n",
       "  'Short Sale'],\n",
       " ['1492832', 'Santa Bay', '909000', '4', '4', '3540', '286.78', 'Short Sale'],\n",
       " ['1499102',\n",
       "  'Thomas Country',\n",
       "  '109900',\n",
       "  '3',\n",
       "  '1',\n",
       "  '1249',\n",
       "  '98.99',\n",
       "  'Short Sale']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = sc.textFile(filepath + \"IntRealEstate.txt\") \\\n",
    "        .zipWithIndex() \\\n",
    "        .filter(lambda x: x[1] > 0) \\\n",
    "        .map(lambda x: x[0]) \\\n",
    "        .map(lambda x: x.split(\"|\"))\n",
    "\n",
    "df2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8506cd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1461262|Arroyo Grande|795000|3|3|2371|365.3|Short Sale',\n",
       " '1478004|Paulo Pablo|399000|4|3|2818|163.59|Short Sale',\n",
       " '1486551|Paulo Pablo|545000|4|3|3032|179.75|Short Sale',\n",
       " '1492832|Santa Bay|909000|4|4|3540|286.78|Short Sale',\n",
       " '1499102|Thomas Country|109900|3|1|1249|98.99|Short Sale']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply filter to get header and data\n",
    "rdd_in = sc.textFile(filepath + \"IntRealEstate.txt\")\n",
    "\n",
    "rdd = rdd_in.filter(lambda x: not x.startswith(\"Property_ID\"))\n",
    "header = rdd_in.filter(lambda x: x.startswith(\"Property_ID\"))\n",
    "\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32043906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1461262',\n",
       "  'Arroyo Grande',\n",
       "  '795000',\n",
       "  '3',\n",
       "  '3',\n",
       "  '2371',\n",
       "  '365.3',\n",
       "  'Short Sale'],\n",
       " ['1478004',\n",
       "  'Paulo Pablo',\n",
       "  '399000',\n",
       "  '4',\n",
       "  '3',\n",
       "  '2818',\n",
       "  '163.59',\n",
       "  'Short Sale'],\n",
       " ['1486551',\n",
       "  'Paulo Pablo',\n",
       "  '545000',\n",
       "  '4',\n",
       "  '3',\n",
       "  '3032',\n",
       "  '179.75',\n",
       "  'Short Sale'],\n",
       " ['1492832', 'Santa Bay', '909000', '4', '4', '3540', '286.78', 'Short Sale'],\n",
       " ['1499102',\n",
       "  'Thomas Country',\n",
       "  '109900',\n",
       "  '3',\n",
       "  '1',\n",
       "  '1249',\n",
       "  '98.99',\n",
       "  'Short Sale']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply flatmap and map\n",
    "rdd1 = rdd.flatMap(lambda x: x.split(\",\")).map(lambda x: x.split(\"|\"))\n",
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "004b2cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Property_ID',\n",
       " 'Location',\n",
       " 'Price',\n",
       " 'Bedrooms',\n",
       " 'Bathrooms',\n",
       " 'Size',\n",
       " 'Price_SQ_FT',\n",
       " 'Status']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the index of each column\n",
    "col_list = header.first().split(\"|\")\n",
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f14d24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = col_list.index(\"Property_ID\")\n",
    "f2 = col_list.index(\"Location\")\n",
    "f3 = col_list.index(\"Price\")\n",
    "f4 = col_list.index(\"Price_SQ_FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0634ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Property_ID|Location|FinalPrice',\n",
       " '1461262|Arroyo Grande|290413500.0',\n",
       " '1478004|Paulo Pablo|65272410.0',\n",
       " '1486551|Paulo Pablo|97963750.0',\n",
       " '1492832|Santa Bay|260683019.99999997',\n",
       " '1499102|Thomas Country|10879001.0',\n",
       " '1489132|Thomas Country|10244910.0',\n",
       " '1467262|Fort Worth|459251100.0',\n",
       " '1478114|Paulo Pablo|91284710.0',\n",
       " '1402551|Nashville|92513750.0',\n",
       " '1405832|San Jose|285160400.0',\n",
       " '1493302|Fort Worth|23858901.0',\n",
       " '1412332|Thomas Country|9544710.0',\n",
       " '1469062|Arroyo Grande|188248200.0',\n",
       " '1498004|Nashville|207972509.99999997',\n",
       " '1586751|Nashville|178976000.0',\n",
       " '1433232|Glendale|213961860.0',\n",
       " '1495502|Fort Worth|44197701.0',\n",
       " '1489100|San Jose|8467728.0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mul_price(d1,d2):\n",
    "    res = float(d1) * float(d2)\n",
    "    return str(res)\n",
    "\n",
    "header_out = header.map(lambda x: x.split(\"|\")[f1] + \"|\" + x.split(\"|\")[f2] + \"|FinalPrice\")\n",
    "rdd2 = rdd1.map(lambda x: x[f1] + \"|\" + x[f2] + \"|\" + mul_price(x[f3],x[f4]) )\n",
    "\n",
    "rdd_out = header_out.union(rdd2)\n",
    "rdd_out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1e6a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we need a single text file as O/P we use coalesce of 1 , which merges all part files\n",
    "rdd_out.coalesce(1).saveAsTextFile(filepath + \"Online/realestate.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
